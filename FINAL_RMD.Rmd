---
title: "prj 3.0"
author: "sasi"
date: "2024-12-03"
output: html_document
---
## 1. Load necessary libraries and read the data

```{r}

setwd("D:/datamining/Airline Delays")
# Load necessary libraries
library(dplyr)


# Load your dataset (update path as needed)
data <- read.csv("D:/datamining/Airline Delays/Dataset 17-flightdelay_Jan_Jun.csv")

# Display initial structure of the dataset
str(data)
head(data)
```

## 2. Handle Missing Values
```{r}
# Specify the variables to keep (including predictors)
selected_vars <- c(
  "PLANE_AGE", "CONCURRENT_FLIGHTS", "PRCP", "DISTANCE_GROUP", "AVG_MONTHLY_PASS_AIRLINE",
  "DEP_DEL15", "DEP_TIME_BLK", "DAY_OF_WEEK", "CARRIER_NAME", "AIRPORT_FLIGHTS_MONTH"
)

# Subset the dataset to retain only these variables
data_clean <- data %>% select(all_of(selected_vars))

## 3. Handle Missing Values
# Check for missing values
missing_values <- colSums(is.na(data_clean))
print(missing_values)

data_clean <- na.omit(data_clean)


```

##3 Remove Duplicates
```{r}
# Remove duplicate rows, if any
data_clean <- data_clean %>% distinct()




```

## 5. Verify Cleaned Dataset
```{r}

# Display structure and summary of cleaned dataset
str(data_clean)
summary(data_clean)

# Check the dimensions of the dataset
cat("Number of rows:", nrow(data_clean), "\n")
cat("Number of columns:", ncol(data_clean), "\n")




```
###6  Visualize data before removing outliers

```{r}


# Load ggplot2 for visualizations
library(ggplot2)

# List of predictors to visualize
predictors <- c("PLANE_AGE", "CONCURRENT_FLIGHTS", "PRCP", "DISTANCE_GROUP", "AVG_MONTHLY_PASS_AIRLINE")

# Visualize each variable with boxplots and histograms
for (var in predictors) {
  # Boxplot
  print(
    ggplot(data_clean, aes_string(y = var)) +
      geom_boxplot(fill = "lightblue") +
      ggtitle(paste("Boxplot of", var)) +
      theme_minimal()
  )
  
  # Histogram
  print(
    ggplot(data_clean, aes_string(x = var)) +
      geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
      ggtitle(paste("Histogram of", var)) +
      theme_minimal()
  )
}

```

## 7. Outlier Detection and Removal (Using IQR Method)
```{r}

# Function to calculate and print outliers for a variable
detect_outliers <- function(data, col) {
  Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)  # First quartile
  Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)  # Third quartile
  IQR <- Q3 - Q1                                   # Interquartile range
  lower_bound <- Q1 - 1.5 * IQR                    # Lower bound
  upper_bound <- Q3 + 1.5 * IQR                    # Upper bound
  
  # Print the bounds
  cat("Variable:", col, "\n")
  cat("  Lower Bound:", lower_bound, "\n")
  cat("  Upper Bound:", upper_bound, "\n")
  
  # Count and display outliers
  outliers <- sum(data[[col]] < lower_bound | data[[col]] > upper_bound, na.rm = TRUE)
  cat("  Number of Outliers:", outliers, "\n\n")
  
  return(list(lower_bound = lower_bound, upper_bound = upper_bound, outliers = outliers))
}

# Apply the function to each predictor
outlier_results <- lapply(predictors, function(var) detect_outliers(data_clean, var))

# Combine results into a data frame for better readability
outlier_summary <- data.frame(
  Variable = predictors,
  Lower_Bound = sapply(outlier_results, function(x) x$lower_bound),
  Upper_Bound = sapply(outlier_results, function(x) x$upper_bound),
  Num_Outliers = sapply(outlier_results, function(x) x$outliers)
)

# Print the summary table
print(outlier_summary)



```


## 8. Visualizing after removing Outliers
```{r}
# Function to remove outliers based on calculated bounds
remove_outliers <- function(data, col, lower_bound, upper_bound) {
  data <- data[data[[col]] >= lower_bound & data[[col]] <= upper_bound, ]
  return(data)
}

# Loop through each predictor to remove outliers
for (i in seq_along(predictors)) {
  var <- predictors[i]
  lower_bound <- outlier_summary$Lower_Bound[i]
  upper_bound <- outlier_summary$Upper_Bound[i]
  
  # Remove outliers
  data_clean <- remove_outliers(data_clean, var, lower_bound, upper_bound)
}

# Visualize the cleaned data
library(ggplot2)

# Step 1: Boxplots and Histograms for each variable
for (var in predictors) {
  # Boxplot
  print(
    ggplot(data_clean, aes_string(y = var)) +
      geom_boxplot(fill = "lightgreen") +
      ggtitle(paste("Boxplot of", var, "After Outlier Removal")) +
      theme_minimal()
  )
  
  # Histogram
  print(
    ggplot(data_clean, aes_string(x = var)) +
      geom_histogram(binwidth = 5, fill = "green", color = "black", alpha = 0.7) +
      ggtitle(paste("Histogram of", var, "After Outlier Removal")) +
      theme_minimal()
  )
}


```

## 5. ploting both

```{r}


```


## 9. Logistic Regression Model

```{r}
# Load necessary libraries
library(caret)      # For confusion matrix and data splitting
library(pROC)       # For ROC and AUC

# Ensure the target variable is a factor
data_clean$DEP_DEL15 <- as.factor(data_clean$DEP_DEL15)

# Split the data into training and testing sets
set.seed(123)
train_index <- createDataPartition(data_clean$DEP_DEL15, p = 0.7, list = FALSE)
train_data <- data_clean[train_index, ]
test_data <- data_clean[-train_index, ]

# Logistic regression model on the cleaned dataset
logistic_model <- glm(DEP_DEL15 ~ ., data = train_data, family = binomial(link = "logit"))

# Summary of the model (coefficients, p-values, etc.)
summary(logistic_model)

# Make predictions on the test data
predicted_prob <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (threshold = 0.5)
predicted_class <- ifelse(predicted_prob > 0.5, 1, 0)

# Confusion matrix
confusion <- confusionMatrix(as.factor(predicted_class), test_data$DEP_DEL15, positive = "1")
print(confusion)

# ROC curve and AUC
roc_curve <- roc(test_data$DEP_DEL15, predicted_prob)
auc_value <- auc(roc_curve)

# Plot the ROC curve
plot(roc_curve, col = "blue", main = "ROC Curve")
abline(a = 0, b = 1, lty = 2, col = "red")
text(0.6, 0.4, paste("AUC =", round(auc_value, 2)), col = "darkred")

# Print AUC value
print(paste("AUC Value:", round(auc_value, 2)))



```


## logistic regresion for significant 
```{r}

# Assign significant columns to a new dataset
dataclean2 <- data_clean[, c("DEP_DEL15", "PLANE_AGE", "CONCURRENT_FLIGHTS", 
                             "PRCP", "DISTANCE_GROUP", "DEP_TIME_BLK", 
                             "CARRIER_NAME")]

# Check if the new dataset has the selected columns
colnames(dataclean2)

# Logistic regression
log_model <- glm(DEP_DEL15 ~ PLANE_AGE + CONCURRENT_FLIGHTS + PRCP + 
                 DISTANCE_GROUP + DEP_TIME_BLK + CARRIER_NAME, 
                 data = dataclean2, family = binomial)

# Summary of the logistic regression model
summary(log_model)


# Install and load necessary packages
install.packages("pROC")
install.packages("caret")
library(pROC)
library(caret)

# Predict probabilities
pred_probs <- predict(log_model, type = "response")

# Generate ROC curve and AUC
roc_curve <- roc(dataclean2$DEP_DEL15, pred_probs)
auc_value <- auc(roc_curve)

# Plot ROC curve
plot(roc_curve, main = "ROC Curve", col = "blue")

# Print AUC value
print(paste("AUC: ", auc_value))

# Confusion matrix
pred_class <- ifelse(pred_probs > 0.5, 1, 0)
conf_matrix <- confusionMatrix(as.factor(pred_class), as.factor(dataclean2$DEP_DEL15))

# Print confusion matrix
print(conf_matrix)



```





##desicion tree

```{r}

# Load necessary packages
library(rpart)
library(rpart.plot)
library(pROC)
library(caret)

# Select significant variables from the logistic regression model
significant_vars <- c("DEP_DEL15", "PLANE_AGE", "CONCURRENT_FLIGHTS", 
                      "PRCP", "DISTANCE_GROUP", "DEP_TIME_BLK", "CARRIER_NAME")

# Create a new dataset with only the significant variables
dataclean2 <- data_clean[, significant_vars]

# Run the Decision Tree model using rpart
dt_model <- rpart(DEP_DEL15 ~ PLANE_AGE + CONCURRENT_FLIGHTS + PRCP + 
                  DISTANCE_GROUP + DEP_TIME_BLK + CARRIER_NAME, 
                  data = dataclean2, method = "class")

# Plot the Decision Tree
rpart.plot(dt_model)

# Make predictions on the data
pred_probs_dt <- predict(dt_model, type = "prob")[, 2]

# Generate ROC curve for Decision Tree
roc_curve_dt <- roc(dataclean2$DEP_DEL15, pred_probs_dt)
auc_value_dt <- auc(roc_curve_dt)

# Plot ROC curve for Decision Tree
plot(roc_curve_dt, main = "ROC Curve - Decision Tree", col = "red")

# Print AUC value for Decision Tree
print(paste("AUC (Decision Tree): ", auc_value_dt))

# Confusion matrix for Decision Tree
pred_class_dt <- ifelse(pred_probs_dt > 0.5, 1, 0)
conf_matrix_dt <- confusionMatrix(as.factor(pred_class_dt), as.factor(dataclean2$DEP_DEL15))

# Print confusion matrix for Decision Tree
print(conf_matrix_dt)




```
```{r}
# Load necessary libraries
library(randomForest)
library(caret)

# Ensure the cleaned dataset is ready
# Assuming data_clean is your cleaned dataset

# Fit the Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(DEP_DEL15 ~ PLANE_AGE + CONCURRENT_FLIGHTS + PRCP + 
                         DISTANCE_GROUP + DEP_TIME_BLK + CARRIER_NAME,
                         data = data_clean,
                         importance = TRUE, ntree = 100)  # Reduced trees to optimize memory

# Print the model summary
print(rf_model)

# Predict on the training data
rf_predictions <- predict(rf_model, data_clean)

# Create confusion matrix
conf_matrix <- confusionMatrix(rf_predictions, data_clean$DEP_DEL15)

# Print the confusion matrix
print(conf_matrix)
```


```{r}
{r}
#Step 1: Libraries Loaded

#To facilitate the analysis, several libraries were loaded:

#dplyr: For data manipulation, including filtering, selecting variables, handling missing values, and removing duplicates.
#ggplot2: Used to visualize data through histograms and boxplots, aiding in understanding data distributions and identifying outliers.
#caret: Provides tools for creating and evaluating machine learning models, splitting datasets, and generating confusion matrices.
#pROC: For evaluating classification models using Receiver Operating Characteristic (ROC) curves and calculating the Area Under the Curve (AUC) as a performance metric.
#rpart and rpart.plot: These tools are used for constructing and visualizing decision trees, which provide interpretable classification results.
#randomForest: Implements a robust ensemble method to improve predictive accuracy by aggregating multiple decision trees.
#Step 2: Loading the Dataset

#The dataset is loaded using the read.csv() function. This dataset contains flight-related attributes such as:

#PLANE_AGE: Age of the aircraft.
#CONCURRENT_FLIGHTS: Number of flights operating simultaneously.
#PRCP: Precipitation levels as an indicator of weather conditions.
#DISTANCE_GROUP: Grouped distance categories for flight lengths.
#AVG_MONTHLY_PASS_AIRLINE: Average number of monthly passengers for the airline.
#DEP_DEL15: Target variable indicating whether a flight was delayed by more than 15 minutes (binary: 1 = delayed, 0 = on-time).
#After loading the data:

#str() inspects the dataset structure to check variable types (e.g., numeric, categorical).
#head() provides a preview of the first few rows to verify successful data loading and ensure variables are formatted as expected.
#Step 3: Data Cleaning and Preprocessing

#Variable Selection: A subset of relevant variables is chosen based on their potential to influence flight delays, improving the efficiency and interpretability of the analysis.
#Handling Missing Values: Missing values are identified and removed using na.omit(), ensuring that the dataset is complete and consistent.
#Duplicate Removal: Any duplicate rows in the dataset are removed to avoid redundancy and bias in the analysis.
#Step 4: Data Visualization

#To better understand the data, visualizations are created for selected predictors:

#Boxplots: Used to identify outliers and understand the spread of numeric variables (e.g., PLANE_AGE, CONCURRENT_FLIGHTS).
#Histograms: Provide a frequency distribution for numeric predictors, helping to detect skewness or unusual patterns.
#Step 5: Outlier Detection and Removal

#Outliers are detected and handled using the Interquartile Range (IQR) method:

#For each predictor, lower and upper bounds are calculated.
#Rows with values outside these bounds are considered outliers and removed. This step ensures that extreme values do not unduly influence the model.
#Step 6: Logistic Regression Modeling

#A logistic regression model is built to predict DEP_DEL15 (delayed or not delayed) using the cleaned dataset.
#A subset of significant predictors is identified and included in a refined logistic regression model for better interpretability.
#Model performance is evaluated using:
#Confusion Matrix: To measure accuracy, sensitivity, and specificity.
#ROC Curve & AUC: To assess the model's classification ability.
#Step 7: Decision Tree Modeling

#A decision tree model is built using rpart. This interpretable model predicts delays based on significant variables like PLANE_AGE and PRCP.
#The tree structure is visualized with rpart.plot for better understanding.
#Model performance is evaluated using a confusion matrix, ROC curve, and AUC.
#Step 8: Random Forest Modeling

#A random forest model is constructed using randomForest to leverage the power of ensemble learning.
#It combines multiple decision trees to improve prediction accuracy and handle variable interactions.
#Variable importance is calculated to identify which factors most influence delays.




```

